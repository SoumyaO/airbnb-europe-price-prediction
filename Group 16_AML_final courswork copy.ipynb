{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **Group 16: airbnb in European cities - Price Prediction**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ImportError",
                    "evalue": "cannot import name 'type_spec_registry' from 'tensorflow.python.framework' (/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/tensorflow/python/framework/__init__.py)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstatsmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutliers_influence\u001b[39;00m \u001b[39mimport\u001b[39;00m variance_inflation_factor \u001b[39mas\u001b[39;00m vif\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearRegression, Ridge, Lasso\n\u001b[0;32m---> 28\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras_tuner\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mkt\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras_tuner/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2019 The KerasTuner Authors\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_tuner\u001b[39;00m \u001b[39mimport\u001b[39;00m applications\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_tuner\u001b[39;00m \u001b[39mimport\u001b[39;00m oracles\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_tuner\u001b[39;00m \u001b[39mimport\u001b[39;00m tuners\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras_tuner/applications/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2019 The KerasTuner Authors\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_tuner\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maugment\u001b[39;00m \u001b[39mimport\u001b[39;00m HyperImageAugment\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_tuner\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mefficientnet\u001b[39;00m \u001b[39mimport\u001b[39;00m HyperEfficientNet\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_tuner\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mresnet\u001b[39;00m \u001b[39mimport\u001b[39;00m HyperResNet\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras_tuner/applications/augment.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_tuner\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi_export\u001b[39;00m \u001b[39mimport\u001b[39;00m keras_tuner_export\n\u001b[1;32m     22\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/api/_v2/keras/__init__.py:12\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_v2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m __internal__\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_v2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m activations\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/__init__.py:21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/models/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m Functional\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/engine/functional.py:26\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtensor\u001b[39;00m \u001b[39mimport\u001b[39;00m layout_map \u001b[39mas\u001b[39;00m layout_map_lib\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m base_layer\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/backend/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m experimental\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39mabs\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39mall\u001b[39m\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/backend/experimental/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m disable_tf_random_generator\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m enable_tf_random_generator\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m is_tf_random_generator_enabled\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/src/__init__.py:21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/src/models/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m Functional\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/src/engine/functional.py:25\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtensor\u001b[39;00m \u001b[39mimport\u001b[39;00m layout_map \u001b[39mas\u001b[39;00m layout_map_lib\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m base_layer\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/src/backend.py:35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute_coordinator_utils \u001b[39mas\u001b[39;00m dc\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtensor\u001b[39;00m \u001b[39mimport\u001b[39;00m dtensor_api \u001b[39mas\u001b[39;00m dtensor\n\u001b[0;32m---> 35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m keras_tensor\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m control_flow_util\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m object_identity\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/src/engine/keras_tensor.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Keras Input Tensor used to track functional API Topology.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m object_identity\n\u001b[1;32m     21\u001b[0m \u001b[39m# isort: off\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m structure\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/src/utils/__init__.py:53\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_file\n\u001b[1;32m     52\u001b[0m \u001b[39m# Preprocessing utils\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_space\u001b[39;00m \u001b[39mimport\u001b[39;00m FeatureSpace\n\u001b[1;32m     55\u001b[0m \u001b[39m# Internal\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayer_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_source_inputs\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/src/utils/feature_space.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m base_layer\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m \u001b[39mimport\u001b[39;00m saving_lib\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m \u001b[39mimport\u001b[39;00m serialization_lib\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/src/engine/base_layer.py:39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m input_spec\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m keras_tensor\n\u001b[0;32m---> 39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m node \u001b[39mas\u001b[39;00m node_module\n\u001b[1;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmixed_precision\u001b[39;00m \u001b[39mimport\u001b[39;00m autocast_variable\n\u001b[1;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmixed_precision\u001b[39;00m \u001b[39mimport\u001b[39;00m policy\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/src/engine/node.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m base_layer_utils\n\u001b[0;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaved_model\u001b[39;00m \u001b[39mimport\u001b[39;00m json_utils\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_utils\n\u001b[1;32m     31\u001b[0m _CONSTANT_VALUE \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_CONSTANT_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n",
                        "File \u001b[0;32m/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/keras/src/saving/legacy/saved_model/json_utils.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaved_model\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m in_tf_saved_model_scope\n\u001b[1;32m     37\u001b[0m \u001b[39m# isort: off\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m type_spec_registry\n\u001b[1;32m     40\u001b[0m _EXTENSION_TYPE_SPEC \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_EXTENSION_TYPE_SPEC\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mEncoder\u001b[39;00m(json\u001b[39m.\u001b[39mJSONEncoder):\n",
                        "\u001b[0;31mImportError\u001b[0m: cannot import name 'type_spec_registry' from 'tensorflow.python.framework' (/opt/anaconda3/envs/python_3_10/lib/python3.10/site-packages/tensorflow/python/framework/__init__.py)"
                    ]
                }
            ],
            "source": [
                "#import tensorflow as tf\n",
                "from IPython.display import display, Image\n",
                "# from xgboost import XGBRegressor\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "from sklearn.linear_model import Ridge, Lasso\n",
                "from sklearn.model_selection import RandomizedSearchCV, train_test_split, GridSearchCV, cross_val_score\n",
                "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error\n",
                "from sklearn import tree\n",
                "import pickle\n",
                "from scipy import stats\n",
                "from sklearn.preprocessing import OneHotEncoder\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.base import BaseEstimator, TransformerMixin\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
                "from scipy.stats import randint\n",
                "import graphviz\n",
                "import xgboost as xgb\n",
                "from xgboost import XGBRegressor\n",
                "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
                "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
                "import keras_tuner as kt\n",
                "import tensorflow as tf\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **1. Data loading and merging**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Dataset Introduction**\n",
                "\n",
                "This dataset offers a comprehensive examination of airbnb prices in popular European cities. \n",
                "\n",
                "It covers a range of attributes for each listing, including room types, cleanliness and satisfaction ratings, number of bedrooms, distance from the city center, and more. The dataset provides a detailed understanding of airbnb prices for both weekdays and weekends. By employing spatial econometric methods, we analyze the factors that influence airbnb prices across these cities.\n",
                "\n",
                "The airbnb in our dataset are located in 10 cities in Europe which include Amsterdam, Athens, Barcelona, Berlin, Rome, Vienna, Paris, London, Lisbon, Budapest.\n",
                "\n",
                "Source: https://www.kaggle.com/datasets/thedevastator/airbnb-prices-in-european-cities\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Methodology**\n",
                "\n",
                "The first step is doing the data cleaning and exploratory data analysis. \n",
                "\n",
                "Next, we implemented different Machine Learning models to predict the airbnb prices:\n",
                "- Decision Tree\n",
                "- Random Forest\n",
                "- XGBoost\n",
                "- Linear Regression\n",
                "- Ridge Regression\n",
                "- Lasso Regression\n",
                "- Neural Network\n",
                "\n",
                "We also applied techniques such as feature selection, target variable log-transformation and hyperparameter tuning to improve model performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data Loading\n",
                "data_files = [\n",
                "    './dataset/amsterdam_weekdays.csv',\n",
                "    './dataset/amsterdam_weekends.csv',\n",
                "    './dataset/athens_weekdays.csv',\n",
                "    './dataset/athens_weekends.csv',\n",
                "    './dataset/barcelona_weekdays.csv',\n",
                "    './dataset/athens_weekends.csv',\n",
                "    './dataset/barcelona_weekdays.csv',\n",
                "    './dataset/barcelona_weekends.csv',\n",
                "    './dataset/berlin_weekdays.csv',\n",
                "    './dataset/berlin_weekends.csv',\n",
                "    './dataset/budapest_weekdays.csv',\n",
                "    './dataset/budapest_weekends.csv',\n",
                "    './dataset/lisbon_weekdays.csv',\n",
                "    './dataset/lisbon_weekends.csv',\n",
                "    './dataset/london_weekdays.csv',\n",
                "    './dataset/london_weekends.csv',\n",
                "    './dataset/paris_weekdays.csv',\n",
                "    './dataset/paris_weekends.csv',\n",
                "    './dataset/rome_weekdays.csv',\n",
                "    './dataset/rome_weekends.csv',\n",
                "    './dataset/vienna_weekdays.csv',\n",
                "    './dataset/vienna_weekends.csv'\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = []\n",
                "\n",
                "for index in range(len(data_files)):\n",
                "    data_file = pd.read_csv(data_files[index])\n",
                "\n",
                "    file_info = data_files[index].split('/')[2].split('_')\n",
                "    \n",
                "    data_file['city'] = file_info[0]\n",
                "    data_file['day'] = file_info[1].split('.')[0]\n",
                "    \n",
                "    df.append(data_file)\n",
                "\n",
                "df = pd.concat(df)\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = df.drop(columns=['Unnamed: 0'])\n",
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = df.reset_index(drop=True)\n",
                "df\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **2. Data Exploration and Cleaning**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **2.1. Dataset summary and initial cleaning**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check missing values\n",
                "# There is no missing values \n",
                "df.isna().sum().sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop duplicates\n",
                "df = df.drop_duplicates()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Over 4000 duplicated rows dropped\n",
                "df = df.reset_index(drop=True)\n",
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save df in a csv file\n",
                "df.to_csv('./dataset/europe_airbnb.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.describe()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **2.2. Inspection on values and distributions of variables**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# visually inspect the data - numerical columns\n",
                "df.hist(bins=50, figsize=(15,10))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Our target variable is realSum - which is highly right skewed. Further investigation on the outliers should be conducted. \n",
                "\n",
                "Other data features:\n",
                "- There's a long tail to a few variables such as attr_index, metro_dist, guest_satisfaction_overall\n",
                "- They are at different scales"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Look at the distributions of log transformed realSum\n",
                "# Convert the realSum to log to reduce the effect of the long tail \n",
                "# and make the distrubtion more gaussian\n",
                "\n",
                "fig, axs = plt.subplots(figsize=(8, 3))\n",
                "sns.histplot(x=np.log1p(df['realSum']), bins=40, ax=axs).set(\n",
                "    xlabel='realSum (Log)',\n",
                "    title='Distribution of realSum (Log)'\n",
                ")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Categorical columns inspection\n",
                "cat_cols = [\n",
                "    'room_type',\n",
                "    'room_shared',\n",
                "    'room_private',\n",
                "    'host_is_superhost',\n",
                "    'city',\n",
                "    'day',\n",
                "    'multi',\n",
                "    'biz'\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribution of categorical columns\n",
                "for col in cat_cols:\n",
                "    fig, ax = plt.subplots(figsize=(5, 2))\n",
                "    sns.countplot(y=col, data=df).set(ylabel=None, title=f'Count of airbnb by {col}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The plots show that:\n",
                "- Most airbnbs are entire home/apt and private rooms. Only a very small number is shared room\n",
                "- Most hosts are not superhosts\n",
                "- The majority of our datasets are airbnbs in London, Rome, Paris and Lisbon. Those are also the capital cities with bigger sizes \n",
                "- The data points of weekdays and weekends are quite equal\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **2.3. Inspection into relationship between variables**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Feature transforms"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['log_realSum'] = np.log1p(df['realSum'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert data types of categorical columns to string\n",
                "df[cat_cols] = df[cat_cols].astype(str)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distributions of categorical columns by Log realSum\n",
                "for col in cat_cols:\n",
                "    fig, ax = plt.subplots(figsize=(8,3))\n",
                "    sns.boxplot(x='log_realSum', y=col, data=df).set(\n",
                "        title=f'Distribution of log price by {col}',\n",
                "    )"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The plots show that:\n",
                "- Prices of entire home/ apt is highest, followed by private room and shared room\n",
                "- If the room is private, price is higher\n",
                "- Prices of airbnb in Amsterdam is higest. The 2nd expensive city is Paris, the 3rd is London\n",
                "- There is no difference of prices between weekdays and weekends, whether the hosts have more than 4 offers or not, and whether the host is superhost or not."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation matrix\n",
                "plt.figure(figsize=(12, 12))\n",
                "sns.heatmap(\n",
                "    df.corr(numeric_only=True), annot=True, cmap=\"RdBu\", annot_kws={\"size\": 15}, vmin=-1, vmax=1, fmt='.1f'\n",
                ")\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Overall, there are a few variables which have high correlations with others. Notably, we have:\n",
                "- log_realSum & attr_index_norm: positive correlation - the higher the attraction index of the airbnb, the higher its price is\n",
                "- log_realSum & persons_capacity: if more people can stay in an airbnb, its price will be higher. This can be similar with the positive correlation with log_realSum and bedrooms\n",
                "- log_realSum & rest_index_norm: if the restaurant index is higher, the airbnb's price will be higher.\n",
                "- attr_index & rest_index: restaurant index is high -> attraction index of the listing will also be high.\n",
                "\n",
                "Multicollinearity exists so data pre-processing to solve this issue is required if we want to use linear regression model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save df in a csv file\n",
                "df.to_csv('airbnb.csv')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **2.4. Data Preparation**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# One hot encoding\n",
                "dfnew = pd.get_dummies(df, drop_first=True, columns=cat_cols, dtype=int)\n",
                "dfnew"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = dfnew.drop(columns=['realSum', 'log_realSum'], axis=1)\n",
                "X"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y1 = dfnew['realSum']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We use the log-transformed price as the target variable\n",
                "y = dfnew['log_realSum']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "num_cols = [\n",
                "    'person_capacity',\n",
                "    'cleanliness_rating',\n",
                "    'guest_satisfaction_overall',\n",
                "    'bedrooms',\n",
                "    'dist',\n",
                "    'metro_dist',\n",
                "    'attr_index',\n",
                "    'attr_index_norm',\n",
                "    'rest_index',\n",
                "    'rest_index_norm',\n",
                "    'lng',\n",
                "    'lat'\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scaling data\n",
                "scale = StandardScaler()\n",
                "X_num_scaled = scale.fit_transform(X[num_cols])\n",
                "\n",
                "X_num_scaled = pd.DataFrame(X_num_scaled, columns=num_cols,\n",
                "                        index=X.index)\n",
                "\n",
                "X_num_scaled"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_cat_cols = X.drop(columns = num_cols)\n",
                "X_cat_cols"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_scaled = pd.concat([X_cat_cols, X_num_scaled], axis=1)\n",
                "X_scaled"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_scaled = pd.concat([X_scaled, y], axis=1)\n",
                "df_scaled.to_csv('prep_data.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_scaled = pd.concat([X_scaled, y, y1], axis=1)\n",
                "df_scaled.to_csv('prep_data_ver2.csv', index=False)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **3. Modelling**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics(y, y_pred):\n",
                "    return {\n",
                "      'MAE': mean_absolute_error(y, y_pred),\n",
                "      'MAPE': mean_absolute_percentage_error(y, y_pred) * 100,\n",
                "      'R2': r2_score(y, y_pred),\n",
                "      'MSE': mean_squared_error(y, y_pred)\n",
                "    }"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**We will use train/test split with a size of 70/30 in all models implementation.**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **3.1. Baseline - Mean Prediction**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The first model is a baseline which only predicts the average premium, regardless of the input variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Test Split\n",
                "X_train_mean, X_test_mean, y_train_mean, y_test_mean = train_test_split(\n",
                "    X_scaled, y, test_size=0.3, random_state=123\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted_mean = y_train_mean.mean()\n",
                "\n",
                "y_pred_train_mean = np.repeat(predicted_mean, len(y_train_mean))\n",
                "y_pred_test_mean = np.repeat(predicted_mean, len(y_test_mean))\n",
                "\n",
                "print('Train: ', compute_metrics(np.expm1(y_train_mean), np.expm1(y_pred_train_mean)))\n",
                "print('Test: ', compute_metrics(np.expm1(y_test_mean), np.expm1(y_pred_test_mean)))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Based on the model performance above, we can see that this baseline is not reliable.\n",
                "The R2 is negative and the MAPE is very high at around 0.49.\n",
                "\n",
                "Hence, more advanced machine learning models should be implemented for this task."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **3.2 Decision Tree**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Based on the characteristics of tree based algorithms, we decided that:\n",
                "- We dropped the columns 'attr_index_norm', 'rest_index_norm' since Decision Tree doesn't require data normalisation and we already have the original values\n",
                "- We dropped 'log_realSum' and used 'realSum' as the decision tree model is no sensitive to outliers\n",
                "\n",
                "Source:\n",
                "https://www.sciencedirect.com/topics/mathematics/decision-tree"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data pre-processing\n",
                "df_dt = dfnew.drop(columns = ['attr_index_norm', 'rest_index_norm', 'log_realSum'], axis = 1)\n",
                "df_dt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify the X and Y\n",
                "dt_X = df_dt.drop(columns=[\"realSum\"], axis = 1)\n",
                "dt_Y = df_dt[\"realSum\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate the dataset into training/testing with percentage 70%/30%\n",
                "dt_x_train, dt_x_test, dt_y_train, dt_y_test = train_test_split(dt_X, dt_Y, test_size=0.3, random_state=123)\n",
                "\n",
                "print(f\"Training on {len(dt_x_train)} observations\", \"\\n\"\n",
                "      f\"Testing on {len(dt_x_test)} observations\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set up the decision tree model\n",
                "dt_model = DecisionTreeRegressor(random_state = 50)\n",
                "\n",
                "# Set the hyper-parameters of the model\n",
                "parameters = {\n",
                "    'max_depth':[3, 5, 8, 10],\n",
                "    'min_samples_split': [2, 3, 5, 8],\n",
                "    'min_samples_leaf': [1, 5, 8, 10],\n",
                "    'splitter':['best','random']\n",
                "}\n",
                "\n",
                "# Perform grid search with cross validation = 5\n",
                "grid_search = GridSearchCV(dt_model, parameters, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train the model and find the best hyper-parameters\n",
                "grid_search.fit(dt_x_train, dt_y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify the best model with the best hyper-parameters\n",
                "best_parameters = grid_search.best_params_\n",
                "print(\"Best Parameters:\", best_parameters)\n",
                "best_model_dt = grid_search.best_estimator_\n",
                "print(\"Best Model:\", best_model_dt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict the test data and calculate the evaluation metrics\n",
                "dt_y_pred = best_model_dt.predict(dt_x_test)\n",
                "print(\"Decision Tree\" \"\\n\", compute_metrics(dt_y_test, dt_y_pred))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualise the tree\n",
                "dot_data = tree.export_graphviz(best_model_dt, out_file=None,\n",
                "                                feature_names= dt_X.columns,\n",
                "                                filled=True, rounded=True,\n",
                "                                special_characters=True)\n",
                "display(graphviz.Source(dot_data))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature importance\n",
                "feature_importance = pd.Series(\n",
                "    best_model_dt.feature_importances_,\n",
                "    index=best_model_dt.feature_names_in_,\n",
                ").sort_values()\n",
                "\n",
                "# Filter feature importance > 0\n",
                "feature_importance = feature_importance[feature_importance > 0]\n",
                "\n",
                "# Plot the feature importance\n",
                "fig, ax = plt.subplots(figsize=(5, 8), dpi=500)\n",
                "bars = ax.barh(feature_importance.index, feature_importance)\n",
                "ax.bar_label(bars)\n",
                "for bars in ax.containers:\n",
                "    ax.bar_label(bars)\n",
                "\n",
                "# Add labels and title\n",
                "plt.ylabel('Feature')\n",
                "plt.xlabel('Weight')\n",
                "plt.title('Feature importance of the Tuned Decision Tree Model')\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The figure above illustrates the significance of various features in determining Airbnb prices in Europe. The most crucial feature, denoted as \"attr_index,\" holds an important value of 0.357111. Following closely is the feature \"bedrooms\" with an importance value of 0.16935, and \"lat\" ranks third with a value of 0.154011. These three features account for more than 65% of the total feature importance in the decision tree model. Therefore, they play a pivotal role in influencing Airbnb prices in Europe.\n",
                "\n",
                "Comparing the performance of the decision tree to the baseline model, both R2 values are negative and the baseline model and decision tree model are -0.02588 and -0.21567 respectively. This indicates that neither of the models is performing well in fitting the data. Therefore, it is advisable to consider implementing other machine learning models.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **3.3 Random Forest**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We used the same set with the decision tree model. Random Forest algorithm exhibits reduced sensitivity to outliers and noisy data compared to other algorithms due to its ensemble nature, which involves averaging predictions from multiple decision trees. Hence, the target variable is the orginial price, not the log-transformed price."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify the X and Y\n",
                "rf_X = df_dt.drop(columns=[\"realSum\"], axis = 1)\n",
                "rf_Y = df_dt[\"realSum\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate the dataset into training/testing with percentage 70%/30%\n",
                "rf_x_train, rf_x_test, rf_y_train, rf_y_test = train_test_split(rf_X, rf_Y, test_size=0.3, random_state=123)\n",
                "\n",
                "print(f\"Training on {len(rf_x_train)} observations\", \"\\n\"\n",
                "      f\"Testing on {len(rf_x_test)} observations\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set up the randome forest model\n",
                "rf_model = RandomForestRegressor(random_state = 889)\n",
                "\n",
                "# Set the hyper-parameters of the model\n",
                "parameters = {\n",
                "    'n_estimators': [50, 200, 300], \n",
                "    'max_features': [2, 4, 6, 8],\n",
                "    'max_depth': [4, 6, 8],\n",
                "    'min_samples_leaf': [0.1, 0.2]\n",
                "}\n",
                "# Perform grid search with cross validation = 5\n",
                "rand_search = RandomizedSearchCV(rf_model, \n",
                "                                parameters,\n",
                "                                cv=5, \n",
                "                                n_jobs=-1,\n",
                "                                scoring='neg_mean_squared_error', \n",
                "                                n_iter=10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train the model and find the best hyper-parameters\n",
                "rand_search.fit(rf_x_train, rf_y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify the best model with the best hyper-parameters\n",
                "best_parameters = rand_search.best_params_\n",
                "print(\"Best Parameters:\", best_parameters)\n",
                "best_rf = rand_search.best_estimator_\n",
                "print(\"Best Model:\", best_rf)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict the test data and calculate the evaluation metrics\n",
                "rf_y_pred = best_rf.predict(rf_x_test)\n",
                "print(\"Random Forest\" \"\\n\", compute_metrics(rf_y_test, rf_y_pred))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature importance\n",
                "feature_importance = pd.Series(\n",
                "    best_rf.feature_importances_,\n",
                "    index=best_rf.feature_names_in_,\n",
                ").sort_values()\n",
                "\n",
                "# Plot the feature importance\n",
                "fig, ax = plt.subplots(figsize=(5, 15), dpi=500)\n",
                "bars = ax.barh(feature_importance.index, feature_importance)\n",
                "ax.set_xlabel('Weight')\n",
                "ax.set_ylabel('Feature')\n",
                "ax.set_title('Feature importance of the Random Forest model')\n",
                "\n",
                "ax.bar_label(bars)\n",
                "for bars in ax.containers:\n",
                "    ax.bar_label(bars)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Compared to the decision tree, the random forest enables better identification of the importance of a feature, and also it is robust to outliers. The influence of outliers is mitigated via the average effect of multiple trees. Moreover, the random forest is well-suited for modeling high-dimensional data due to its ability to handle missing values, as well as its versatility in accommodating various data types, including continuous, categorical, and binary variables. Also, the random forest is strong enough to overcome the overfitting problems without pruning the trees due to the bootstrapping and ensemble scheme.\n",
                "\n",
                "The above figure illustrates the significance of features in the random forest model. Among the features, \"lat\" holds the highest importance value of 0.281221, followed by \"lng\" and \"person_capacity\" with importance values of 0.195311 and 0.129157 respectively. These three features contribute to over 60% of the total feature importance in the random forest model. Furthermore, the prominence of latitude and longitude highlights their significant impact on airbnb prices.\n",
                "\n",
                "Comparing the performance of the random forest model to the decision tree model and baseline model, a positive R2, which is 0.13243 indicates a relatively better fit and some degree of relationship between the random forest model and Airbnb prices. However, it is worth noting that the R2 value of 0.13243 still suggests room for improvement in accurately capturing the data patterns because the range of R2 is from 0 to 1, with values closer to 1 indicating a stronger fit between the model and the data.\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **3.4 XGBoost**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4.1 XGBoost with original target variable"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify the X and Y\n",
                "xgb_prepared = df_dt.drop(columns=[\"realSum\"], axis = 1)\n",
                "xgb_labels = df_dt[\"realSum\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split the data into training and testing sets\n",
                "# These are the same training and test sets as used in Decision Tree and Random Forest models\n",
                "X_train, X_test, y_train, y_test = train_test_split(xgb_prepared, xgb_labels, test_size=0.3, random_state=123)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the parameter grid for grid search\n",
                "param_grid = {\n",
                "    'max_depth': [2, 3, 4, 5],\n",
                "    'learning_rate': [0.1, 0.01, 0.001],\n",
                "    'n_estimators': [100, 200, 300],\n",
                "}\n",
                "\n",
                "# Create the XGBoost regressor\n",
                "model = XGBRegressor(objective='reg:squarederror')\n",
                "\n",
                "# Perform grid search with cross-validation\n",
                "xgb_rand_search = RandomizedSearchCV(\n",
                "    model, \n",
                "    param_grid, \n",
                "    cv=5, \n",
                "    n_jobs=-1,\n",
                "    scoring='neg_mean_squared_error')\n",
                "\n",
                "xgb_rand_search.fit(X_train, y_train)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify the best model with the best hyper-parameters\n",
                "best_parameters = xgb_rand_search.best_params_\n",
                "print(\"Best Parameters:\", best_parameters)\n",
                "best_model_xgb = xgb_rand_search.best_estimator_\n",
                "print(\"Best Model:\", best_model_xgb)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make predictions\n",
                "xgb_y_pred = best_model_xgb.predict(X_test)\n",
                "\n",
                "print(\"XGBoost\" \"\\n\", compute_metrics(y_test, xgb_y_pred))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature importance\n",
                "feature_importance = pd.Series(\n",
                "    best_model_xgb.feature_importances_,\n",
                "    index=best_model_xgb.feature_names_in_,\n",
                ").sort_values()\n",
                "\n",
                "# Filter feature importance > 0\n",
                "feature_importance = feature_importance[feature_importance > 0]\n",
                "\n",
                "# Plot the feature importance\n",
                "fig, ax = plt.subplots(figsize=(5, 8), dpi=500)\n",
                "bars = ax.barh(feature_importance.index, feature_importance)\n",
                "ax.bar_label(bars)\n",
                "for bars in ax.containers:\n",
                "    ax.bar_label(bars)\n",
                "\n",
                "# Add labels and title\n",
                "plt.ylabel('Feature')\n",
                "plt.xlabel('Weight')\n",
                "plt.title('Feature importance of the XGBoost Model \\n with original target variable')\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4.2 XGBoost with log-transformed target variable"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The presence of outliers in the context of boosting can be detrimental due to the construction of each subsequent tree based on the residuals or errors of the previous trees. Outliers tend to exhibit substantially larger residuals compared to non-outliers, consequently causing gradient boosting to allocate a disproportionate emphasis on these data points."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Therefore, it is worth to be considered whether handling the skewness in the target variable can improve the performance of XGBoost model or not. Instead of removing the outliers, we can try to log transform the target variable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify the X and Y\n",
                "xgb_prepared = df_dt.drop(columns=[\"realSum\"], axis = 1)\n",
                "xgb_labels_2 = dfnew[\"log_realSum\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split the data into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(xgb_prepared, xgb_labels_2, test_size=0.3, random_state=123)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the parameter grid for grid search\n",
                "param_grid = {\n",
                "    'max_depth': [2, 3, 4, 5],\n",
                "    'learning_rate': [0.1, 0.01, 0.001],\n",
                "    'n_estimators': [100, 200, 300],\n",
                "}\n",
                "\n",
                "# Create the XGBoost regressor\n",
                "model = XGBRegressor(objective='reg:squarederror')\n",
                "\n",
                "# Perform grid search with cross-validation\n",
                "xgb_rand_search = RandomizedSearchCV(\n",
                "    model, \n",
                "    param_grid, \n",
                "    cv=5, \n",
                "    n_jobs=-1,\n",
                "    scoring='neg_mean_squared_error')\n",
                "\n",
                "xgb_rand_search.fit(X_train, y_train)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify the best model with the best hyper-parameters\n",
                "best_parameters = xgb_rand_search.best_params_\n",
                "print(\"Best Parameters:\", best_parameters)\n",
                "best_model_xgb = xgb_rand_search.best_estimator_\n",
                "print(\"Best Model:\", best_model_xgb)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make predictions\n",
                "xgb_y_pred = best_model_xgb.predict(X_test)\n",
                "\n",
                "print(\"XGBoost with log transformation\" \"\\n\", compute_metrics(y_test, xgb_y_pred))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('XGBoost with log transformation: \\n ', compute_metrics(np.expm1(y_test), np.expm1(xgb_y_pred)))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "From the result above, the MSE does not reduce significantly compared with the 1st XGBoost model's MSE. \n",
                "\n",
                "However, the MAE and MAPE and R2 of the 2nd XGBoost are all better, which indicates an overall better performance than the 1st XGBoost."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature importance\n",
                "feature_importance = pd.Series(\n",
                "    best_model_xgb.feature_importances_,\n",
                "    index=best_model_xgb.feature_names_in_,\n",
                ").sort_values()\n",
                "\n",
                "# Filter feature importance > 0\n",
                "feature_importance = feature_importance[feature_importance > 0]\n",
                "\n",
                "# Plot the feature importance\n",
                "fig, ax = plt.subplots(figsize=(5, 8), dpi=500)\n",
                "bars = ax.barh(feature_importance.index, feature_importance)\n",
                "ax.bar_label(bars)\n",
                "for bars in ax.containers:\n",
                "    ax.bar_label(bars)\n",
                "\n",
                "# Add labels and title\n",
                "plt.ylabel('Feature')\n",
                "plt.xlabel('Weight')\n",
                "plt.title('Feature importance of the XGBoost Model with \\n log transformation of the target variable')\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4.3 XGBoost with original target variable and feature selection"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Since there are features that have high correlation with the other, feature selection is considered when running the third XGBoost. We tried this method with the original price, not the price with log transformation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reading datasets\n",
                "df = pd.read_csv(\"./prep_data_ver2.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Replace logtransformed price by original price\n",
                "df = df.drop(columns = [\"log_realSum\"], axis = 1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Seperate features and labels\n",
                "xgb_prepared = df.drop(columns=[\"realSum\"], axis = 1)\n",
                "xgb_labels = df[\"realSum\"]\n",
                "\n",
                "# Split the data into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(xgb_prepared, xgb_labels, test_size=0.3, random_state=42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create and train the XGBoost regressor\n",
                "model = XGBRegressor(objective='reg:squarederror')  # Use 'reg:squarederror' for regression\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "predictions = model.predict(X_test)\n",
                "\n",
                "# Evaluate the performance using mean squared error (MSE)\n",
                "mse = mean_squared_error(y_test, predictions)\n",
                "print(\"Mean Squared Error:\", mse)\n",
                "\n",
                "# Get feature importance scores\n",
                "importance_scores = model.feature_importances_\n",
                "feature_names = X_train.columns\n",
                "\n",
                "# Sort the feature importance scores and feature names in descending order\n",
                "sorted_indices = importance_scores.argsort()[::-1]\n",
                "sorted_scores = importance_scores[sorted_indices]\n",
                "sorted_feature_names = [feature_names[i] for i in sorted_indices]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "mse_results = []  # Store MSE values for different values of k\n",
                "k_values = range(1, len(feature_names) + 1)  # Range of k values to consider\n",
                "\n",
                "for k in k_values:\n",
                "    # Select top k features based on importance scores\n",
                "    selected_features = sorted_feature_names[:k]\n",
                "\n",
                "    # Seperate selected features\n",
                "    X_train_selected = X_train[selected_features]\n",
                "    X_test_selected = X_test[selected_features]\n",
                "\n",
                "    # Create and train the XGBoost regressor with selected features\n",
                "    model_selected = XGBRegressor(objective='reg:squarederror')\n",
                "    model_selected.fit(X_train_selected, y_train)\n",
                "\n",
                "    # Make predictions using the model with selected features\n",
                "    predictions_selected = model_selected.predict(X_test_selected)\n",
                "\n",
                "    # Calculate mean squared error (MSE) with selected features\n",
                "    mse_selected = mean_squared_error(y_test, predictions_selected)\n",
                "\n",
                "    # Store the MSE value\n",
                "    mse_results.append(mse_selected)\n",
                "\n",
                "# Find the index with the minimum MSE value\n",
                "optimal_k_index = mse_results.index(min(mse_results))\n",
                "optimal_k = k_values[optimal_k_index]\n",
                "\n",
                "print(\"MSE Results:\", mse_results)\n",
                "print(\"Optimal k:\", optimal_k)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select optimal k values based on mse\n",
                "k = optimal_k  # Number of top features to select\n",
                "selected_features = sorted_feature_names[:k]\n",
                "print(\"Selected Features:\", selected_features)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Refit the model based on the optimal k value\n",
                "# Seperate selected features\n",
                "X_train_selected = X_train[selected_features]\n",
                "X_test_selected = X_test[selected_features]\n",
                "\n",
                "# Create and train the XGBoost regressor with selected features\n",
                "model_selected = XGBRegressor(objective='reg:squarederror')\n",
                "model_selected.fit(X_train_selected, y_train)\n",
                "\n",
                "# Make predictions using the model with selected features\n",
                "predictions_selected = model_selected.predict(X_test_selected)\n",
                "\n",
                "# Evaluate the performance using mean squared error (MSE) with selected features\n",
                "mse_selected = mean_squared_error(y_test, predictions_selected)\n",
                "print(\"Mean Squared Error (Selected Features):\", mse_selected)\n",
                "# Calculate R-squared\n",
                "r_squared = r2_score(y_test, predictions_selected)\n",
                "\n",
                "print(\"R-squared:\", r_squared)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mae = mean_absolute_error(y_test, predictions_selected)\n",
                "print(\"Mean Absolute Error:\", mae)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the parameter grid for randomize search\n",
                "param_grid = {\n",
                "    'max_depth': [3, 5, 7],\n",
                "    'learning_rate': [0.1, 0.01, 0.001],\n",
                "    'n_estimators': [100, 300, 500],\n",
                "}\n",
                "\n",
                "# Create the XGBoost regressor\n",
                "model = XGBRegressor(objective='reg:squarederror')\n",
                "\n",
                "# Perform grid search with cross-validation\n",
                "grid_search = RandomizedSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
                "grid_search.fit(X_train_selected, y_train)\n",
                "\n",
                "# Get the best model\n",
                "best_model = grid_search.best_estimator_\n",
                "\n",
                "# Make predictions\n",
                "predictions = best_model.predict(X_test_selected)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Evaluate the performance using mean squared error (MSE)\n",
                "# mse = mean_squared_error(y_test, predictions)\n",
                "# print(\"Mean Squared Error:\", mse)\n",
                "\n",
                "# # Print the best hyperparameters\n",
                "# print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
                "\n",
                "# # Make predictions\n",
                "# xgb_y_pred = best_model.predict(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Seperate selected features\n",
                "X_train_selected = X_train[selected_features]\n",
                "X_test_selected = X_test[selected_features]\n",
                "\n",
                "# Create and train the XGBoost regressor with selected features\n",
                "model_selected = XGBRegressor(objective='reg:squarederror')\n",
                "model_selected.fit(X_train_selected, y_train)\n",
                "\n",
                "# Make predictions using the model with selected features\n",
                "predictions_selected = model_selected.predict(X_test_selected)\n",
                "\n",
                "# Evaluate the performance using mean squared error (MSE) with selected features\n",
                "mse_selected = mean_squared_error(y_test, predictions_selected)\n",
                "print(\"Mean Squared Error (Selected Features):\", mse_selected)\n",
                "print(\"XGBoost with feature selection\" \"\\n\", compute_metrics(y_test, predictions_selected))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualise feature importance\n",
                "importance_scores = model_selected.feature_importances_\n",
                "feature_names = X_train_selected.columns\n",
                "\n",
                "# Sort the feature importance scores and feature names in descending order\n",
                "sorted_indices = importance_scores.argsort()[::-1]\n",
                "sorted_scores = importance_scores[sorted_indices]\n",
                "sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
                "\n",
                "# Create a feature importance plot\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.bar(range(len(importance_scores)), sorted_scores)\n",
                "plt.xticks(range(len(importance_scores)), sorted_feature_names, rotation='vertical')\n",
                "plt.xlabel('Features')\n",
                "plt.ylabel('Importance Scores')\n",
                "plt.title('Feature Importance')\n",
                "plt.tight_layout()  # Adjust spacing\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "####  **Model performance**\n",
                "The technique of feature selection is applied based on the feature importance generated by XGBoost. An iteration is used to find the optimum number of features using Mean Squared Error (MSE) as the metric. The optimum k is 13 and the corresponding MSE is 54884.70 and the R-squared value is 0.5537.\n",
                "\n",
                "####  **Business insights**\n",
                "From a business perspective, the analysis provides valuable insights into the factors that significantly impact accommodation prices. Understanding these influential features can assist businesses in several ways.\n",
                "\n",
                "Firstly, it allows accommodation providers to set competitive pricing strategies by considering the factors that potential guests value the most. For example, according to the chart 'Feature Importance' we can see that  accommodations with private rooms, a higher number of bedrooms and being close to attractions may command a premium price. Additionally, businesses can focus on enhancing specific attributes or amenities to increase guest satisfaction and overall value perception.\n",
                "\n",
                "Furthermore, the analysis can help businesses identify potential market opportunities. For instance, if the 'city_barcelona' feature has a significant impact on prices, it suggests that accommodations in Barcelona may have higher market demand or specific characteristics that attract guests, enabling businesses to tailor their marketing and operational strategies accordingly."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **3.5 Linear Regression**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(\"prep_data_ver2.csv\")\n",
                "df.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Split train test and validation dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Set X and y dataset separately\n",
                "X = df.drop([\"log_realSum\", \"realSum\"], axis = 1)\n",
                "y = df[\"log_realSum\"]\n",
                "y_real = df[\"realSum\"]\n",
                "print(X.shape, y.shape, y_real.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Split train, test, validation dataset\n",
                "# split train, other dataset from original X and y (RealSum)\n",
                "X_train, X_other, y_train_real, y_other = train_test_split(\n",
                "    X, y_real, test_size=0.3, random_state=11\n",
                "    )\n",
                "# split test, validation dataset from X_other and y_other\n",
                "X_test, X_valid, y_test_real, y_valid_real = train_test_split(\n",
                "    X_other, y_other, test_size=0.3, random_state=11\n",
                "    )\n",
                "# split train, other dataset from original X and y (log_RealSum)\n",
                "X_train, X_other, y_train, y_other = train_test_split(\n",
                "    X, y, test_size=0.3, random_state=11\n",
                "    )\n",
                "# split test, validation dataset from X_other and y_other\n",
                "X_test, X_valid, y_test, y_valid = train_test_split(\n",
                "    X_other, y_other, test_size=0.3, random_state=11\n",
                "    )\n",
                "# check the shape\n",
                "print(X_train.shape, X_test.shape, X_valid.shape)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.5.1 Use VIF (Variance Inflation Factor) to identify features that have multicollinearity problem"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 3.5.1.1 Check the VIF values for all features and filter those to be eliminated"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Calculate VIF of each features\n",
                "# create dataframe\n",
                "df_vif = pd.DataFrame()\n",
                "df_vif[\"Feature\"] = X.columns\n",
                "df_vif[\"VIF\"] = [vif(X.values, i) for i in range(X.shape[1])]\n",
                "print(df_vif)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Eliminate features with high VIF values\n",
                "name_remove = df_vif.loc[df_vif[\"VIF\"] > 5, \"Feature\"]\n",
                "name_remove"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 3.5.1.2 Check the VIF values for the selected features"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Strategy 1: keep coordinates remove cities**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Calculate VIF of each features\n",
                "# In strategy 1: cities would be removed and coordinates would be kept\n",
                "# create dataframe\n",
                "df_vif_left = pd.DataFrame()\n",
                "# create the list of feature names to be excluded - combined with the manually eliminating process based on the feature descriptions\n",
                "l_exclude = [\"bedrooms\", \"dist\", \"attr_index\", \"rest_index\", \"room_type_Private room\", \"room_type_Shared room\", \n",
                "             'city_athens', 'city_barcelona', 'city_berlin', 'city_budapest', 'city_lisbon', 'city_london', 'city_paris', 'city_rome', 'city_vienna']\n",
                "X_left = X.drop(l_exclude, axis = 1)\n",
                "\n",
                "df_vif_left[\"Feature\"] = X_left.columns\n",
                "df_vif_left[\"VIF\"] = [vif(X_left.values, i) for i in range(X_left.shape[1])]\n",
                "print(df_vif_left)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Strategy 2: keep cities remove coordinates**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Calculate VIF of each features\n",
                "# In strategy 2: coordinates would be removed and cities would be kept\n",
                "# create dataframe\n",
                "df_vif_left = pd.DataFrame()\n",
                "# create the list of feature names to be excluded - combined with the manually eliminating process based on the feature descriptions\n",
                "l_exclude = [\"bedrooms\", \"dist\", \"attr_index\", \"rest_index\", \"room_type_Private room\", \"room_type_Shared room\", \n",
                "             'lat', 'lng']\n",
                "X_left = X.drop(l_exclude, axis = 1)\n",
                "\n",
                "df_vif_left[\"Feature\"] = X_left.columns\n",
                "df_vif_left[\"VIF\"] = [vif(X_left.values, i) for i in range(X_left.shape[1])]\n",
                "print(df_vif_left)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It can be concluded that both removing cities removing coordinates help reduce feature colinearity."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.5.2 Implement Linear regression models\n",
                "For the basic models, they don't perform cross validations so only the train and test datasets are used."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 3.5.2.1 Linear Regression model with all features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Check dependent variables\n",
                "print(y_test, y_test_real)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Fit the basic linear model with log-transformation\n",
                "linear_m = LinearRegression()\n",
                "linear_m.fit(X_train, y_train)\n",
                "y_pred_linear = linear_m.predict(X_test)\n",
                "\n",
                "#### Fit the basic linear model without log-transformation\n",
                "linear_m_real = LinearRegression()\n",
                "linear_m_real.fit(X_train, y_train_real)\n",
                "y_pred_linear_real = linear_m_real.predict(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Linear Regression with Log-transformation on y after transformed back: \\n ', compute_metrics(np.expm1(y_test), np.expm1(y_pred_linear)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Linear Regression without Log-transformation: \\n ', compute_metrics( y_test_real, y_pred_linear_real))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.scatter(y_test, y_pred_linear)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.scatter(y_test_real, np.expm1(y_pred_linear))\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.scatter(y_test_real, y_pred_linear_real)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Show the coefficients\n",
                "# Create a dataframe of coefficient names and values\n",
                "coe_linear = pd.DataFrame()\n",
                "coe_linear[\"Feature\"] = linear_m.feature_names_in_\n",
                "coe_linear[\"Value\"] = linear_m.coef_\n",
                "# Sort the coefficients\n",
                "coe_linear = coe_linear.sort_values(by = [\"Value\"], ascending = False)\n",
                "# Visualise the coefficients\n",
                "fig, ax = plt.subplots()\n",
                "ax.bar(coe_linear[\"Feature\"], coe_linear[\"Value\"])\n",
                "ax.set_xticklabels(coe_linear[\"Feature\"], rotation = 90, fontsize = 7)\n",
                "\n",
                "#[i+\": \"+j.astype(str) for i, j in zip(linear_m.feature_names_in_, linear_m.coef_)]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The first linear model is trained on all features, and there are two features that have much higher coefficients than others. It should also be noted that model 1 without log- transformation on response variables (MAE 95.6, MAPE 39.3, MSE 46148.3, R-square 0.351) performed worse than the one on the log-transformed data (MAE 75.6, MAPE 25.7, MSE 43048.8, R- square 0.395). This result is aligned with the insight concluded from the previous visualisations. \n",
                "\n",
                "We decided that the group of linear regression models would all be trained and evaluated on the log-transformed response variables."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 3.5.2.2 Linear regression model without collinearity data.\n",
                "According to the correlation matrix implemented at the data preprocessing stage, it can be inferred that many numerical features have high correlations with the other.\n",
                "\n",
                "To reduce the model bias, features that may have high collinearities to other features are eliminated."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Strategy 1: keep coordinates remove cities**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Fit the basic linear model after removing the features that have collinearity or are hard to interpret\n",
                "# according to the correlation matrix, the following features will be removed:\n",
                "# In strategy 1: cities would be removed and coordinates would be kept\n",
                "l_remove = [\"bedrooms\", \"dist\", \"attr_index\", \"rest_index\", \"room_type_Private room\", \"room_type_Shared room\", \n",
                "             'city_athens', 'city_barcelona', 'city_berlin', 'city_budapest', 'city_lisbon', 'city_london', 'city_paris', 'city_rome', 'city_vienna']\n",
                "X_train_remove = X_train.drop(columns = l_remove, axis = 1)\n",
                "X_test_remove = X_test.drop(columns = l_remove, axis = 1)\n",
                "X_train_remove.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Fit the basic linear model\n",
                "linear_m_remove = LinearRegression()\n",
                "linear_m_remove.fit(X_train_remove, y_train)\n",
                "y_pred_linear_remove = linear_m_remove.predict(X_test_remove)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Linear Regression with Log-transformation on coordinates: \\n ', compute_metrics(np.expm1(y_test), np.expm1(y_pred_linear_remove)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Show the coefficients\n",
                "coe_linear = pd.DataFrame()\n",
                "coe_linear[\"Feature\"] = linear_m_remove.feature_names_in_\n",
                "coe_linear[\"Value\"] = linear_m_remove.coef_\n",
                "# Sort the coefficients\n",
                "coe_linear = coe_linear.sort_values(by = [\"Value\"], ascending = False)\n",
                "# Visualise the coefficients\n",
                "fig, ax = plt.subplots()\n",
                "ax.bar(coe_linear[\"Feature\"], coe_linear[\"Value\"])\n",
                "ax.set_xticklabels(coe_linear[\"Feature\"], rotation = 90, fontsize = 7)\n",
                "\n",
                "#[i+\": \"+j.astype(str) for i, j in zip(linear_m_remove.feature_names_in_, linear_m_remove.coef_)]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Strategy 2: keep cities remove coordinates**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Fit the basic linear model after removing the features that have collinearity or are hard to interpret\n",
                "# according to the correlation matrix, the following features will be removed:\n",
                "# In strategy 2: coordinates would be removed and cities would be kept\n",
                "l_remove = [\"bedrooms\", \"dist\", \"attr_index\", \"rest_index\", \"room_type_Private room\", \"room_type_Shared room\", \n",
                "             'lat', 'lng']\n",
                "X_train_remove = X_train.drop(columns = l_remove, axis = 1)\n",
                "X_test_remove = X_test.drop(columns = l_remove, axis = 1)\n",
                "X_train_remove.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Fit the basic linear model\n",
                "linear_m_remove = LinearRegression()\n",
                "linear_m_remove.fit(X_train_remove, y_train)\n",
                "y_pred_linear_remove = linear_m_remove.predict(X_test_remove)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Linear Regression with Log-transformation on cities: \\n ', compute_metrics(np.expm1(y_test), np.expm1(y_pred_linear_remove)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Show the coefficients\n",
                "coe_linear = pd.DataFrame()\n",
                "coe_linear[\"Feature\"] = linear_m_remove.feature_names_in_\n",
                "coe_linear[\"Value\"] = linear_m_remove.coef_\n",
                "# Sort the coefficients\n",
                "coe_linear = coe_linear.sort_values(by = [\"Value\"], ascending = False)\n",
                "# Visualise the coefficients\n",
                "fig, ax = plt.subplots()\n",
                "ax.bar(coe_linear[\"Feature\"], coe_linear[\"Value\"])\n",
                "ax.set_xticklabels(coe_linear[\"Feature\"], rotation = 90, fontsize = 7)\n",
                "\n",
                "#[i+\": \"+j.astype(str) for i, j in zip(linear_m_remove.feature_names_in_, linear_m_remove.coef_)]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Both strategies work for the linear regression model, the coefficients are making sense. MAE, MSE and adjusted R-squared score are better in the 2nd model which keeps cities and removes the coordinates (lat, lng)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Combined with the correlation matrix from the EDA, the model result indicates that some features may have multicollinearity in the dataset. To further explore the relationship between features, VIF is introduced to identify correlated features.\n",
                "\n",
                "In the feature selection process, cities and coordinates are selected as those with high VIF values, and one of them must be excluded from the dataset. Both excluding strategies are implemented, and the model that was trained on data with cities (MAE 78.4, MAPE 26.5, MSE 45218.4, R-square 0.364) performs better than the one with coordinates (MAE 89.3, MAPE 30.6, MSE 49558.2, R-square 0.303). The coefficients are also easier to interpret compared to the first linear model. Eventually, the one trained on cities is kept as the second linear model. Comparing to the first model, the second model seems to have reasonable coefficients, at the expense of a slightly lower model performance."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Regularization**\n",
                "\n",
                "\n",
                "Aside from eliminating the correlated features, adding a regularization term will also help reduce model bias.\n",
                "\n",
                "Ridge regression and Lasso regression were applied in the next parts."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.5.3 Ridge Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Set the range of Grid Search\n",
                "param_grid = {'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Fit the linear model with Ridge regularisation\n",
                "ridge_m = Ridge()\n",
                "# 5-fold cross-validation\n",
                "ridge_m = GridSearchCV(ridge_m, param_grid, cv=5, n_jobs=-1)\n",
                "ridge_m.fit(X_train, y_train)\n",
                "# check the value of alpha\n",
                "best_alpha_ridge = ridge_m.best_params_['alpha']\n",
                "print(f\"Best alpha for Ridge Regression: {best_alpha_ridge}\")\n",
                "# predict the test result\n",
                "y_pred_ridge = ridge_m.predict(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Ridge Regression with Log-transformation: \\n ', compute_metrics(np.expm1(y_test), np.expm1(y_pred_ridge)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Show the coefficients\n",
                "coe_linear = pd.DataFrame()\n",
                "coe_linear[\"Feature\"] = ridge_m.best_estimator_.feature_names_in_\n",
                "coe_linear[\"Value\"] = ridge_m.best_estimator_.coef_\n",
                "# Sort the coefficients\n",
                "coe_linear = coe_linear.sort_values(by = [\"Value\"], ascending = False)\n",
                "# Visualise the coefficients\n",
                "fig, ax = plt.subplots()\n",
                "ax.bar(coe_linear[\"Feature\"], coe_linear[\"Value\"])\n",
                "ax.set_xticklabels(coe_linear[\"Feature\"], rotation = 90, fontsize = 7)\n",
                "\n",
                "#[i+\": \"+j.astype(str) for i, j in zip(ridge_m.best_estimator_.feature_names_in_, ridge_m.best_estimator_.coef_)]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.5.4 Lasso Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Fit the linear model with Lasso regularisation\n",
                "lasso_m = Lasso()\n",
                "# 5-fold cross-validation\n",
                "lasso_m = GridSearchCV(lasso_m, param_grid, cv=5, n_jobs=-1)\n",
                "lasso_m.fit(X_train, y_train)\n",
                "# check the value of alpha\n",
                "best_alpha_ridge = ridge_m.best_params_['alpha']\n",
                "print(f\"Best alpha for Ridge Regression: {best_alpha_ridge}\")\n",
                "# predict the test result\n",
                "y_pred_lasso = lasso_m.predict(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Lasso Regression with Log-transformation: \\n ', compute_metrics(np.expm1(y_test), np.expm1(y_pred_lasso)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### Show the coefficients\n",
                "coe_linear = pd.DataFrame()\n",
                "coe_linear[\"Feature\"] = lasso_m.best_estimator_.feature_names_in_\n",
                "coe_linear[\"Value\"] = lasso_m.best_estimator_.coef_\n",
                "# Sort the coefficients\n",
                "coe_linear = coe_linear.sort_values(by = [\"Value\"], ascending = False)\n",
                "# Visualise the coefficients\n",
                "fig, ax = plt.subplots()\n",
                "ax.bar(coe_linear[\"Feature\"], coe_linear[\"Value\"])\n",
                "ax.set_xticklabels(coe_linear[\"Feature\"], rotation = 90, fontsize = 7)\n",
                "\n",
                "#[i+\": \"+j.astype(str) for i, j in zip(lasso_m.best_estimator_.feature_names_in_, lasso_m.best_estimator_.coef_)]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Both regularization models are trained on all features, since these models are used for addressing the multi-collinearity problem . In addition, the Grid-Search method with cross validation was applied for hyperparameter tuning."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The two regularised models do not vary much with regard to the model performance (Ridge: MAE 75.6, MAPE 25.7, MSE 43056.1, R-square 0.395, Lasso: MAE 76.1, MAPE 25.9, MSE 43352.5, R-square 0.391), when the Ridge model slightly outperformed the Lasso model. In terms of the feature coefficients, Lasso’s coefficients vary within a smaller range."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **3.6 MutliLayer Perceptron (MLP)**\n",
                "MutliLayer Perceptron (MLP): We have tried the MultiLayer Pereceptron here to see if the neural network will handle the non-linear relationships well by learning mappings between input features and price outputs. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_network = pd.read_csv('prep_data_ver2.csv')\n",
                "# Using the log of realSum to address the skewness\n",
                "X = df_network.drop(columns=['realSum', 'log_realSum'])\n",
                "# we dont need the normalized index columns as we have applied our own scaling to normalize the original index columns\n",
                "X.drop(columns=['attr_index_norm', 'rest_index_norm'], inplace=True)\n",
                "y = df_network['log_realSum']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split the dataset into train-val-test (40-30-30%)\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
                "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=123)\n",
                "\n",
                "print(\"Training shape: {}\".format(X_train.shape))\n",
                "print(\"Validation shape: {}\".format(X_val.shape))\n",
                "print(\"Testing shape: {}\".format(X_test.shape))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train.columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the model\n",
                "def train_model(hp):    \n",
                "    num_units_l1 = hp.Int('num_units_l1', min_value = 200, max_value=260) \n",
                "    num_units_l2 = hp.Int('num_units_l2', min_value = 150, max_value=200) \n",
                "    num_units_l3 = hp.Int('num_units_l3', min_value = 30, max_value=50) \n",
                "    num_units_l4 = hp.Int('num_units_l4', min_value = 10, max_value=20) \n",
                "    # num_units_l5 = hp.Int('num_units_l5', min_value = 3, max_value=5) \n",
                "\n",
                "\n",
                "    dropout_rate = hp.Float('dropout_rate', min_value = 0.1, max_value=0.3) \n",
                "    learning_rate = hp.Float('learning_rate', min_value = 0.001, max_value=0.1, sampling='log') \n",
                "    \n",
                "    model = tf.keras.models.Sequential([\n",
                "        tf.keras.layers.Dense(num_units_l1, activation=\"relu\", input_shape=[X_train.shape[1]]),\n",
                "        tf.keras.layers.Dropout(dropout_rate),\n",
                "        tf.keras.layers.Dense(num_units_l2, activation=\"relu\"),\n",
                "        tf.keras.layers.Dense(num_units_l3, activation=\"relu\"),\n",
                "        tf.keras.layers.Dense(num_units_l4, activation=\"relu\"),\n",
                "        # tf.keras.layers.Dense(num_units_l5, activation=\"relu\"),\n",
                "        tf.keras.layers.Dense(1)])\n",
                "    \n",
                "    model.compile(  optimizer=tf.keras.optimizers.experimental.Adam(learning_rate = learning_rate),\n",
                "                    loss='mean_squared_error',\n",
                "                    metrics = [\"mean_squared_error\", \"mean_absolute_error\", \"mean_absolute_percentage_error\"])\n",
                "    \n",
                "    return model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import datetime\n",
                "current_timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
                "\n",
                "# Define the tuner\n",
                "tuner = kt.Hyperband(train_model,\n",
                "                     objective='val_mean_squared_error',\n",
                "                     max_epochs=15,\n",
                "                     factor=3,\n",
                "                     directory='logs',\n",
                "                     project_name='aml_' + current_timestamp)\n",
                "# Perform the hypertuning\n",
                "tuner.search(X_train, y_train, validation_data=(X_val,y_val))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get the best hyperparameters from the tuner\n",
                "best_hps = tuner.get_best_hyperparameters()[0]\n",
                "print(\"Best number of hidden units:\", best_hps['num_units_l1'])\n",
                "print(\"Best number of hidden units:\", best_hps['num_units_l2'])\n",
                "print(\"Best number of hidden units:\", best_hps['num_units_l3'])\n",
                "print(\"Best number of hidden units:\", best_hps['num_units_l4'])\n",
                "# print(\"Best number of hidden units:\", best_hps['num_units_l5'])\n",
                "print(\"Best dropout rate:\", best_hps['dropout_rate'])\n",
                "print(\"Best learning rate:\", best_hps['learning_rate'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train the model\n",
                "best_model = tuner.hypermodel.build(best_hps)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
                "best_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=early_stopping_cb)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate the model\n",
                "net_y_pred = best_model.predict(X_test)\n",
                "\n",
                "# Calculate performance metrics\n",
                "print('Network with log transformation: \\n ', compute_metrics(np.expm1(y_test), np.expm1(net_y_pred.reshape(-1))))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract the encoded features from the best autoencoder model\n",
                "encoder = tf.keras.Sequential(best_model_ae.layers[:4])\n",
                "X_train_encoded = encoder.predict(X_train)\n",
                "X_val_encoded = encoder.predict(X_val)\n",
                "X_test_encoded = encoder.predict(X_test)\n",
                "\n",
                "# Define the MLP model for price prediction\n",
                "def train_model(hp):    \n",
                "    num_units_l1 = hp.Int('num_units_l1', min_value = 10, max_value=15) \n",
                "    num_units_l2 = hp.Int('num_units_l2', min_value = 5, max_value=10)  \n",
                "    dropout_rate = hp.Float('dropout_rate', min_value = 0.1, max_value=0.3) \n",
                "    learning_rate = hp.Float('learning_rate', min_value = 0.001, max_value=0.1, sampling='log') \n",
                "    \n",
                "    model = tf.keras.models.Sequential([\n",
                "        tf.keras.layers.Dense(num_units_l1, activation=\"relu\", input_shape=[X_train_encoded.shape[1]]),\n",
                "        tf.keras.layers.Dropout(dropout_rate),\n",
                "        tf.keras.layers.Dense(num_units_l2, activation=\"relu\"),\n",
                "        tf.keras.layers.Dense(1)])\n",
                "    \n",
                "    model.compile(  optimizer=tf.keras.optimizers.experimental.Adam(learning_rate = learning_rate),\n",
                "                    loss='mean_squared_error',\n",
                "                    metrics = [\"mean_squared_error\", \"mean_absolute_error\", \"mean_absolute_percentage_error\"])\n",
                "    return model\n",
                "\n",
                "current_timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
                "\n",
                "# Define the tuner\n",
                "tuner_mlp = kt.Hyperband(train_model,\n",
                "                     objective='val_mean_squared_error',\n",
                "                     max_epochs=5,\n",
                "                     factor=3,\n",
                "                     directory='logs',\n",
                "                     project_name='aml_' + current_timestamp)\n",
                "# Perform the hypertuning\n",
                "tuner_mlp.search(X_train_encoded, y_train, validation_data=(X_val_encoded, y_val))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **3.7 Autoencoder**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here, we tried another neural network 'Autoencoder' relevant to our dataset to see if this model would give us better performance for price prediction in terms of lesser MSE and a better R2. We will train the Autoencoder and use the encoder part to extract the latent representation of new input data (dimension reduction). Next, we feed this to the MLP model for price prediction and see if this approach works better."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the model\n",
                "def train_model(hp):    \n",
                "    num_units_l1 = hp.Int('num_units_l1', min_value = 20, max_value=25) \n",
                "    num_units_l2 = hp.Int('num_units_l2', min_value = 15, max_value=20) \n",
                "    num_units_l3 = hp.Int('num_units_l3', min_value = 5, max_value=15) \n",
                "\n",
                "    dropout_rate = hp.Float('dropout_rate', min_value = 0.1, max_value=0.3) \n",
                "    learning_rate = hp.Float('learning_rate', min_value = 0.001, max_value=0.1, sampling='log') \n",
                "    \n",
                "    # Encoder\n",
                "    encoder = tf.keras.models.Sequential([ \n",
                "        tf.keras.layers.Dense(num_units_l1, activation=\"relu\", input_shape=[X_train.shape[1]]),\n",
                "        tf.keras.layers.Dropout(dropout_rate),\n",
                "        tf.keras.layers.Dense(num_units_l2, activation=\"relu\"),\n",
                "        tf.keras.layers.Dense(num_units_l3, activation=\"relu\"),\n",
                "    ])\n",
                "\n",
                "    # Decoder\n",
                "    decoder = tf.keras.models.Sequential([\n",
                "        tf.keras.layers.Dense(num_units_l2, activation=\"relu\"),\n",
                "        tf.keras.layers.Dense(num_units_l1, activation=\"relu\"),\n",
                "        tf.keras.layers.Dense(X_train.shape[1])\n",
                "    ])\n",
                "\n",
                "    model = tf.keras.models.Sequential([encoder, decoder])\n",
                "    \n",
                "    model.compile(  optimizer=tf.keras.optimizers.experimental.Adam(learning_rate = learning_rate),\n",
                "                    loss='mean_squared_error',\n",
                "                    metrics = [\"mean_squared_error\", \"mean_absolute_error\", \"mean_absolute_percentage_error\"])\n",
                "    \n",
                "    return model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import datetime\n",
                "current_timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
                "\n",
                "# Define the tuner\n",
                "tuner_ae = kt.Hyperband(train_model,\n",
                "                     objective='val_mean_squared_error',\n",
                "                     max_epochs=5,\n",
                "                     factor=3,\n",
                "                     directory='logs',\n",
                "                     project_name='aml_' + current_timestamp)\n",
                "# Perform the hypertuning\n",
                "tuner_ae.search(X_train, X_train, validation_data=(X_val, X_val))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get the best hyperparameters from the tuner\n",
                "best_hps_ae = tuner_ae.get_best_hyperparameters()[0]\n",
                "print(\"Best number of hidden units:\", best_hps_ae['num_units_l1'])\n",
                "print(\"Best number of hidden units:\", best_hps_ae['num_units_l2'])\n",
                "print(\"Best number of hidden units:\", best_hps_ae['num_units_l3'])\n",
                "print(\"Best dropout rate:\", best_hps_ae['dropout_rate'])\n",
                "print(\"Best learning rate:\", best_hps_ae['learning_rate'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train the autoencoder with the best hyperparameters\n",
                "best_model_ae = tuner_ae.hypermodel.build(best_hps_ae)\n",
                "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
                "best_model_ae.fit(X_train, X_train, epochs=30, validation_data=(X_val, X_val), callbacks=early_stopping_cb)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract the encoded features from the best autoencoder model\n",
                "encoder = tf.keras.Sequential(best_model_ae.layers[:4])\n",
                "X_train_encoded = encoder.predict(X_train)\n",
                "X_val_encoded = encoder.predict(X_val)\n",
                "X_test_encoded = encoder.predict(X_test)\n",
                "\n",
                "# Define the MLP model for price prediction\n",
                "def train_model(hp):    \n",
                "    num_units_l1 = hp.Int('num_units_l1', min_value = 10, max_value=15) \n",
                "    num_units_l2 = hp.Int('num_units_l2', min_value = 5, max_value=10)  \n",
                "    dropout_rate = hp.Float('dropout_rate', min_value = 0.1, max_value=0.3) \n",
                "    learning_rate = hp.Float('learning_rate', min_value = 0.001, max_value=0.1, sampling='log') \n",
                "    \n",
                "    model = tf.keras.models.Sequential([\n",
                "        tf.keras.layers.Dense(num_units_l1, activation=\"relu\", input_shape=[X_train_encoded.shape[1]]),\n",
                "        tf.keras.layers.Dropout(dropout_rate),\n",
                "        tf.keras.layers.Dense(num_units_l2, activation=\"relu\"),\n",
                "        tf.keras.layers.Dense(1)])\n",
                "    \n",
                "    model.compile(  optimizer=tf.keras.optimizers.experimental.Adam(learning_rate = learning_rate),\n",
                "                    loss='mean_squared_error',\n",
                "                    metrics = [\"mean_squared_error\", \"mean_absolute_error\", \"mean_absolute_percentage_error\"])\n",
                "    return model\n",
                "\n",
                "current_timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
                "\n",
                "# Define the tuner\n",
                "tuner_mlp = kt.Hyperband(train_model,\n",
                "                     objective='val_mean_squared_error',\n",
                "                     max_epochs=5,\n",
                "                     factor=3,\n",
                "                     directory='logs',\n",
                "                     project_name='aml_' + current_timestamp)\n",
                "# Perform the hypertuning\n",
                "tuner_mlp.search(X_train_encoded, y_train, validation_data=(X_val_encoded, y_val))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get the best hyperparameters\n",
                "best_hps_mlp = tuner_mlp.get_best_hyperparameters()[0]\n",
                "print(\"Best number of hidden units:\", best_hps_mlp['num_units_l1'])\n",
                "print(\"Best number of hidden units:\", best_hps_mlp['num_units_l2'])\n",
                "print(\"Best dropout rate:\", best_hps_mlp['dropout_rate'])\n",
                "print(\"Best learning rate:\", best_hps_mlp['learning_rate'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train the MLP \n",
                "best_model_mlp = tuner_mlp.hypermodel.build(best_hps_mlp)\n",
                "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
                "best_model_mlp.fit(X_train_encoded, y_train, epochs=30, validation_data=(X_val_encoded, y_val), callbacks=early_stopping_cb)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate the model\n",
                "net2_y_pred = best_model_mlp.predict(X_test_encoded)\n",
                "\n",
                "# Calculate performance metrics\n",
                "print('Network with log transformation: \\n ', compute_metrics(np.expm1(y_test), np.expm1(net2_y_pred.reshape(-1))))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In order to predict prices without explicit engineering of features, we employed a deep neural network in the form of a Multi-Layer Perceptron (MLP). The MLP was trained using supervised learning with its inputs being the independent variables of the dataset and its expected output being the price of the listing. We use MSE as the loss function for training. Hyperparameters of the network such as the number of neurons in each layer, learning rate and dropout are computed through hyperparameter tuning. With the MLP we aim to implicitly capture the complex relationship between the dependent and the independent variable. The model did not perform well, which could be due to issues with the data/features or the presence of outliers. As there is a lack of direct interpretability of the importance of individual features, it becomes difficult to spot the problem. \n",
                "\n",
                "\n",
                "In another approach, we tried an Autoencoder to compress the input data into a lower dimensional latent space representation. This representation can capture the essential features and patterns relevant to price prediction. The Autoencoder was trained with the input being the independent variables in order to reconstruct the same. Once the training is done, only the encoder is used to extract the compressed features from the dataset. These features are then input to another MLP which predicts the prices. However, this model does not outperform the MLP.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **4. Business Insights and Applications**\n",
                "This project can provide valuable insights and assistance to Airbnb owners in several ways. Firstly, by predicting Airbnb prices in European cities, owners can gain a better understanding of the factors that influence pricing and make data-driven decisions when setting their own rental rates. They can adjust their pricing strategy based on the identified significant features, such as the number of bedrooms, location coordinates, and other relevant attributes. This can help them optimize their pricing to attract more guests while ensuring competitive rates.\n",
                "\n",
                "Furthermore, the project can help Airbnb owners identify the peak seasons and high-demand periods in specific cities. With this knowledge, owners can strategically adjust their availability and pricing during these times to maximize their occupancy rates and revenue. They can offer higher rates during peak seasons when demand is high and adjust prices accordingly during low-demand periods to maintain a steady flow of guests. This approach can lead to improved financial performance and better utilization of their rental properties.\n",
                "\n",
                "Moreover, the insights from the project can guide Airbnb owners in enhancing their property listings. By understanding the features that have a significant impact on prices, owners can focus on improving those aspects of their accommodations. For example, they can invest in upgrading bedrooms, adding amenities, or highlighting proximity to attractions or popular landmarks. This can help them attract more bookings, increase customer satisfaction, and potentially command higher prices.\n",
                "\n",
                "Additionally, the project's findings can assist Airbnb owners in making data-informed decisions regarding property investments. By analyzing the key factors that influence Airbnb prices, owners can evaluate potential properties for purchase or rental with a better understanding of their income-generating potential. They can identify locations and property characteristics that align with the features driving higher prices, which can guide their investment decisions and maximize their return on investment."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **4. Conclusions and Recommendations**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Linear regression offers direct interpretability but assumes linearity and requires a high level of data pre-processing, especially in this task due to the multicollinearity issue. Meanwhile, decision trees and random forests provide intermediate interpretability and handle non-linear relationships but they are not as performant as the XGBoost models. Neural network models excel in non-linearity but can be computationally intensive and complicated to interpret results. XGBoost strikes a balance by offering strong performance, robustness to outliers, and moderate interpretability.\n",
                "\n",
                "Among other models being used in this project, **the third XGBoost model with feature selection** stands out with its high predictive accuracy, built-in feature selection, and ability to capture non-linear relationships. It has the highest R2, and its MSE is only higher than those of linear regression models that we tried. Hence, we decided to choose this model's results as insights for strategic business analysis and implications in this case.\n",
                "\n",
                "In addition, to enhance the insights that this analysis can bring, more information can be collected and scraped from the airbnb website and added to the dataset, such as: the number of reviews, ratings, detailed of reviews, property description (house rules, safety, cancellation policy), number of bookings per month, pricing structure, etc. These information can be helpful to analyse whether the pricing of a property is suitable or not, factors that contribute to a popular airbnb, what customers care about, etc.\n",
                "\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "conda_3_10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
